% -*- LaTeX -*- Time-stamp: <06/10/18 10:38:28 ptr>

\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage{pslatex}
\usepackage{pstricks}
\usepackage[dvips]{epsfig}
%\usepackage[dvipdf]{graphics}
\usepackage[dvips]{graphics}
\usepackage[american]{babel}

\usepackage{epic}
\usepackage{eepic}
\usepackage{psfig}

\usepackage{listings}

\lstset{language=C++,basicstyle=\small,numbers=left,numberstyle=\tiny,stepnumber=1,numbersep=5pt,stringstyle=\ttfamily,keywordstyle=\ttfamily\bfseries}

\providecommand{\STLport}{{\fontfamily{cmss}\selectfont STLport}}
\providecommand{\libstd}{{\fontfamily{cmtt}\selectfont GNU \mbox{libstdc++}}}
\newcommand{\CPP}{\mbox{{C}{+}{+}}}

\title{Comparison of Strings Implementations \\ in \CPP{} language}
\author{Petr Ovtchenkov\footnote{\copyright{} Petr Ovtchenkov, 2003--2006. Licensed under the Academic Free License version 3{.}0.}}

\begin{document}

\maketitle

\begin{abstract}
This article present comparison of STL strings with copy-on-write and
non-copy-on-write argorithms, based on \STLport{} strings, ropes
and \libstd{} implementations.

Some related issues, like fstream and stringstream performance
are also under consideration.

I expect that this results will help to make decision between
STL implementations as well proper choice of strings implementation
and STL usage technique.
\end{abstract}

\tableofcontents

\section{Computers}

In the tests was used following computers and operational envirinments:
\begin{enumerate}
  \item Tyan Tiger motherboard, two $1.33$-GHz
        AMD$^{\mbox{\tiny\textregistered}}$ Athlon$^{\mbox{\tiny TM}}$ XP 1500+ processors under Linux (kernel 2{.}6{.}16{.}26, \verb|glibc| 2{.}3{.}6);\label{AMD}
  \item Two $2.6$-GHz AMD$^{\mbox{\tiny\textregistered}}$ Opteron$^{\mbox{\tiny TM}}$ 252 processors under Linux (kernel 2{.}6{.}15, x86\_64, \verb|glibc| 2{.}3{.}6)\label{AMD64}
\end{enumerate}

\section{Compilers}

In tests was used GNU gcc 3{.}4{.}4 (on AMD64) or 4{.}1{.}1 (on AMD)
with appropriate
\verb|libstdc++| libraries (version~3).

\section{Time Measure\label{timemeasure}}

Due to ``time'' function has different options and output format on Linux
and other UINIXes, I use program \texttt{time} from
\texttt{complement}\footnote{\texttt{http://complement.sourceforge.net}, see appropriate SVN repository on SourceForge.}
project bundle\footnote{Portability note: system should has \texttt{wait3} function.}.
By the way this function provide high-precision time measure.

\section{Statistic}

The measure accuracy
depends upon program load time, the constant measure drift, common computer load (by other processes) and time measure inaccuracy, the random measure drifts.
To reduce influence as the constant measure drift as the random measure drift, the time of test should not to be too short.

Every experiment repeate $10$~times, to get more-or-less acceptable statistic.
For every series of results I do ordinal statistical manipulation.
Mean time is
\begin{equation}
\bar t = \sum_{i = 1}^{n} t_i,
\end{equation}
where $t_i$ is time measure for test $i$.
Mean square deviation
\begin{equation}
\sigma^2 = \frac{1}{n}\sum_{i = 1}^{n} (t_i - \bar t)^2
\end{equation}
or
\begin{equation}
\sigma^2 = \frac{1}{n}\sum_{i = 1}^{n} t_i^2 - \left(\frac{1}{n}\sum_{i = 1}^{n} t_i\right)^2
\end{equation}
or
\begin{equation}
\sigma^2 = \frac{1}{n} \left(\sum_{i = 1}^{n} t_i^2 - 
                        \frac{1}{n}\left(\sum_{i = 1}^{n} t_i\right)^2\right)\label{incrDev}
\end{equation}
The equation \ref{incrDev} give algorithm of one-pass incremental calculation
both mean value $\bar t$ and it mean square deviation $\sigma^2$.
Small mean square deviation let us to be ensure that results are gain our trust.

Parameters in the tests are chosen so the time of test itself was greater than program
load time/unload time, but still to be acceptable for experimentator's patience.


\section{The Tests Descriptions\label{TestsDescr}}

All tests are ``synthetic'', to show the diference in programming
technique.

\subsection{Add Characters to String\label{incr-string}
(test \#1)}

We have empty string. In the loop we add to this string single
character.
Really this is a test for algorithm
of memory allocation (and moving memory contents to reallocated string).

\lstinputlisting{string/add/str.cc}

\subsection{Search of Substring\label{search-string} (test \#2)}

We have string. In the loop we search three substrings.
The matched substrings are positioned in
the beginning, middle and end of the string. All searches are
successful.

\lstinputlisting{string/find/str.cc}

\subsection{Mixed Operations\label{mixed-string} (test \#3)}

This is a mix of common operations under strings: assignment,
search of substring, replace of substring by another substring,
concatenation of strings.

\lstinputlisting{string/ops/str.cc}

\begin{figure}
  \begin{center}
    \input strings.tex
  \end{center}
  \caption{STL strings implementations comparison. Single thread. $t$ is a wall time
           for tests. On X-axis you see test's number (see text).
           The implementation of strings in \STLport{} (non-COW)
           drammaticaly faster than COW implementations (\libstd{} and
           ropes in \STLport{}) in tests that modify strings.
           But if the test only copy string, the COW implementation show better
           performance.
           \label{STLport-gpp-strings}}
\end{figure}

\begin{figure}
  \begin{center}
    \input amd-amd64.tex
  \end{center}
  \caption{Progress of systems over 3 years: two processors systems. See platforms description
           on page~\pageref{AMD}. This graphics show that progress was not only in
           frequency (computer \#2 has twice frequency over \#1, but most times
           are more then twice better),
           but in core architecture too (or this is compiler?)\label{amd-amd64}}
\end{figure}

\subsection{String copy\label{params-string} (test \#4)}

This test intended to hilight the cost of strings copy (jusr copy, without modifications).
Implementations of strings that use copy-on-write algorithms
expected to show better results here.


\lstinputlisting{string/params/str.cc}

You can see (fig.~\ref{STLport-gpp-strings}) that \libstd{} show better
results than \STLport{} in this test. The reason is that \libstd{}
use COW algorithm: during copy operation
there are no memory allocation occur---two strings refer to the same
memory chunk (like \verb|tmp| and \verb|par| on line~$9$). This test
has three copy operations (lines~$7$, $9$ and $20$).

\subsection{String copy again\label{params-ref-string} (test \#5)}

This is a variant of test above (\ref{params-string}), but use more realistic
parameter pass (by reference).

\lstinputlisting{string/params-ref/str.cc}

As in test above (\ref{params-string}) the \libstd{} show better time
by the same reason. But let's draw attention to the tendency: I remove
$1/3$ of string copy operations (two string copy operations remains in the test, no copy of strings while pass
parameter into function), and wall time of this test decline $1/3$ too.

If I will pass only const reference to string, then I will avoid any copy
operations and a difference in time between two approaches will disappear.

\subsection{Short string copy\label{params-short-string} (tests \#6 and \#7)}

This is a variant of test above (\ref{params-string}), but use ``short'' string
parameter pass. This test show effect of ``short string optimization'' technique
in \STLport{}. This technique use short buffer in the ``string'' instance,
this allow to skip memory allocation for short strings.

\lstinputlisting{string/params-short/str.cc}

Test \#6 is default \STLport{}, with ``short string optimization'', while
test \#7 present results without ``short string optimization''\footnote{define
\texttt{\_STLP\_DONT\_USE\_SHORT\_STRING\_OPTIM}}. In case of \libstd{}
tests \#6 and \#7 are the same.

\STLport{} was build with ``short string'' size 16. ``Short strings optimization'' give
test time $3\%$ longer then without ones (for strings with size 20, this test case
not shown on figures).
But you see, for ``short'' strings test with ``short'' strings optimization
show time that $64\%$ better!

\subsection{String proxy objects\label{add-string-proxy} (tests \#8 and \#9)}

The agregation of strings using the \verb|+| operator is an expensive operation
as it requires construction of temporary objects that need memory allocation
and deallocation. The problem can be even more important if you are adding
several strings together in a single expression. To avoid this problem \STLport{}
implement expression template. With this technique addition of 2 strings is not
a string anymore but a temporary object having a reference to each of the
original strings involved in the expression. This object carry information
directly to the destination string to set its size correctly and only make
a single call to the allocator. This technique also works for the addition of
$N$ elements where elements are \verb|basic_string|, \verb|C| string or a
single character.

The drawback can be longer compilation time and bigger executable size.

Another problem is that some compilers (gcc) fail to use string proxy object
if do with class derived from string.

Let's try to estimate the benefits from string proxy technique with following test.

\lstinputlisting{string/add-proxy/str.cc}

Test \#8 is default \STLport{}, in test \#9 temporary string objects
in use\footnote{define \texttt{\_STLP\_USE\_TEMPLATE\_EXPRESSION} turn on}.
For \libstd{} both \#8 and \#9 are the same test.


The figure~\ref{STLport-gpp-strings} show that \STLport{} implementation
in comparison with \libstd{}
give better results even without expression templates. But with
expression templates the results are better more then twice.

\section{Role of Allocators}

Note: in the \verb|glibc| 2{.}3{.}6 enhancement of memory allocation
(over \verb|glibc| 2{.}2{.}5) leads to
advantage of \STLport{}'s \verb|node_alloc| became insignificant.

The \STLport{} provide default ``optimized'' memory allocator
(\verb|node_alloc|). This allocator was used when I run strings tests
for \STLport{} (see section~\ref{TestsDescr}). May be the win of \STLport{} is
due to advanced memory allocation technique?

In the \STLport{} implemented three base memory allocators:
\begin{itemize}
  \item ``optimized'' \verb|node_alloc|;
  \item adapter around \verb|new| operator;
  \item adapter around \verb|malloc| call.
\end{itemize}

\begin{figure}
  \begin{center}
    \input alloc.tex
  \end{center}
  \caption{Role of memory allocator in strings implementation (\STLport).
           Wall time for tests. \label{STLport-alloc-strings}}
\end{figure}

\begin{figure}
  \begin{center}
    \input alloc-2_2_5.tex
  \end{center}
  \caption{Role of memory allocator in strings implementation (\STLport), \texttt{glibc} 2{.}2{.}5, compiler GNU gcc 3{.}4{.}4. 
           Wall time for tests.
  The hardware are the same as for results on figure~\ref{STLport-alloc-strings}.
  \label{STLport-alloc-ii-ii-v-strings}}
\end{figure}

Let's repeat tests from section~\ref{TestsDescr} for \STLport{} with different
allocators. We see (fig.~\ref{STLport-alloc-strings}) that all allocators
are good enough
%: only on test with lot amount of memory
%allocation/reallocation
%operations (test~1) the ``optimized'' \verb|node_alloc| has a tiny advantage
%(difference between best and worst allocators is $3\%$).
For search operations (test~2) the results as expected
are the same (within measure of inaccuracy).

The tests 4 and 5 show
that cost of \verb|malloc| system call---\verb|node_alloc| reuse once
allocated memory. But copy of string's content still present (compare with time of
\libstd{} on fig.~\ref{STLport-gpp-strings}). In the past (\verb|glibc| 2{.}2{.}5,
compiler GNU gcc 3{.}4{.}4)
the cost of \verb|malloc| was significant, but now it near zero (compare graphics
\ref{STLport-alloc-strings} and \ref{STLport-alloc-ii-ii-v-strings}).

The hardware are the same on figures~\ref{STLport-alloc-strings}
and \ref{STLport-alloc-ii-ii-v-strings}, different Linux kernel
(2{.}6{.}16{.}26 vs 2{.}6{.}12{.}5), different
glibc (2{.}3{.}6 vs 2{.}2{.}5) and different versions of compiler (gcc 4{.}1{.}1 vs 3{.}4{.}4).

Tests for parameters pass (by referenece, \#5) give the same results, in tests
\#7 (short strings, no ``short string optimization'') older implementation slightly win.
But in other tests fresh implementation is much better!

\section{Strings in Multithreaded Environment}

Memory allocation performance in single and multithreaded environments
is an important aspect of any application. The work with \CPP{}
strings in multithreaded environments is highly depends upon
underlying allocator.

The tests are the same as described in section~\ref{TestsDescr},
except that every test run simultaneously in two threads.


\subsection{Comparison of \STLport{} and \libstd{} in multi-threaded applications}

\begin{figure}
  \begin{center}
    \input strings-MT.tex
  \end{center}
  \caption{Tests wall time (\libstd{} and \STLport{}
           strings) in multi-threaded environment (two threads).
           \label{STLport-gpp-MT-strings}}
\end{figure}

\begin{figure}
  \begin{center}
    \input strings-MT2.tex
  \end{center}
  \caption{Tests wall time (\libstd{} and \STLport{}
           strings) in multi-threaded environment (two threads) (same as fig.~\ref{STLport-gpp-MT-strings}, another scale).
           \label{STLport-gpp-MT-strings2}}
\end{figure}

The results (fig.~\ref{STLport-gpp-MT-strings}, \ref{STLport-gpp-MT-strings2}) are near
the same as shown on figure~\ref{STLport-gpp-strings}
except that for mix operations test (test~3, described in
section~\ref{mixed-string}) the \STLport{} rope's performance
degradate too much. In previous releases of \libstd{} it
performance was worse (significant) than rope's but in present
implementation it good enough.

\subsection{Time Profile in \libstd}

\begin{figure}
  \begin{center}
    \input strings-MT-t-libstd.tex
  \end{center}
  \caption{
           Wall time, user time and system time per thread for 
           \libstd{} in multi-threaded environment.
           \label{gpp-MT-strings-t}}
\end{figure}

On figure~\ref{gpp-MT-strings-t} you can see user, system and wall
time for tests with \libstd{} strings implementation.
We see that both threads remain in user space almost all time.
This may be due to a lot of waiting state. 

As we can see in test~3, the general performance problem
is seems in a lot of thread synchronization operation
(a lot of system time, that is greater than
user time, in test~3 I can associate only with thread synchronization primitives).



\subsection{Time Profile in \STLport}

\begin{figure}
  \begin{center}
    \input strings-MT-t-stlp-n.tex
  \end{center}
  \caption{
           Wall time, user time and system time per thread for 
           \STLport{} (node allocator) in multi-threaded environment.
           \label{STLport-MT-strings-t}}
\end{figure}

\begin{figure}
  \begin{center}
    \input strings-MT-t-stlp-m.tex
  \end{center}
  \caption{
           Wall time, user time and system time per thread for 
           \STLport{} (malloc allocator) in multi-threaded environment.
           \label{STLport-MT-strings-m}}
\end{figure}

\begin{figure}
  \begin{center}
    \input strings-MT-t-stlp-r.tex
  \end{center}
  \caption{
           Wall time, user time and system time per thread for 
           \STLport{} ropes in multi-threaded environment.
           \label{STLport-MT-strings-r}}
\end{figure}

Figure~\ref{STLport-MT-strings-t} present user and system time per thread
and wall time for tests with STLport with
\texttt{node\_alloc}-based,
figure~\ref{STLport-MT-strings-m} show profile for tests with \texttt{malloc}-based allocators, and figure~\ref{STLport-MT-strings-r} for ropes.

The surprise for me was that \texttt{malloc}-based variants
win (in $1.5--2$ times faster) over \texttt{node\_alloc}-based variant in tests \#3,
\#4 and \#5.
This fact can be explained by usage of memory allocated chunks vector
in \texttt{node\_alloc}. This vector accessed from different threads
and such access should use thread synchronization primitives.
Compare figures~\ref{STLport-alloc-strings} and \ref{STLport-gpp-MT-strings}.
The tests~1--2 has the same time in multi-threaded and single-threaded context for
\texttt{malloc}-based and \texttt{node\_alloc}-based allocators.
In test~3 \texttt{malloc}-based variant climb down, while
\texttt{node\_alloc}-based keep position. 

\section{Strings vs. Ropes}

Some time ago, there are many discussions about complexity
of string assignment algorithm.
In this time 
SGI made experimental implementation of standard string interfaces
with constant copy/insert/replace algorithm complexity---the \texttt{ropes}.

Really \texttt{ropes} and \texttt{strings} has different usage scope.
I use STLport implementation (that come from SGI \texttt{string} and \texttt{rope}
classes) to compare ones. You can see STLport story here:
\texttt{http://stlport.sourceforge.net/History.shtml}.
Some words about \texttt{string} and \texttt{rope}
from first hands you can find here:\\
\texttt{http://www.sgi.com/tech/stl/string\_discussion.html}.

This test based on mix of copy, insert, append and replace
operations under classic strings or ropes. The main part
of test you can see on lines 23--32 (where \texttt{T} is
either \texttt{string} or \texttt{rope}).

\lstinputlisting{string/ropes/str.cc}

\begin{figure}
  \begin{center}
    \input str-rope.tex
  \end{center}
  \caption{Strings vs. Ropes.\label{STLport-rope-string}}
\end{figure}

\begin{figure}
  \begin{center}
    \input str-rope-1.tex
  \end{center}
  \caption{Strings vs. Ropes.\label{STLport-rope-string-1}}
\end{figure}

\begin{figure}
  \begin{center}
    \input str-rope-2.tex
  \end{center}
  \caption{Strings vs. Ropes.\label{STLport-rope-string-2}}
\end{figure}

As expected, efforts to establish constant assign/insert/replace
algorithm complexity lead to
overhead---so ropes are preferable if you want to process
long strings (longer then 12K, really depends upon compiler's optimization quality), as you can see on figures~\ref{STLport-rope-string} and~\ref{STLport-rope-string-1}.
The strings implementation in \libstd{} show better results beginning from size 1K
(figures~\ref{STLport-rope-string}--\ref{STLport-rope-string-2}).

Evident, that complexity of \texttt{string} operations is near the linear
(as in \STLport{} as in \libstd{} implementations),
while \texttt{rope} has constant complexity. But the overhead
of \texttt{rope} is significant, so cost of constant algorithm complexity
is high enough.

%Note, that behaviour of \texttt{rope} and \libstd{} \texttt{strings} implementation
%alike in single-thread environment (fig.~\ref{STLport-gpp-strings}).

\section{The Stream Tests Descriptions\label{StreamTestsDescr}}

\begin{figure}
  \begin{center}
    \input streams.tex
  \end{center}
  \caption{Streams.\label{streams}}
\end{figure}

\begin{figure}
  \begin{center}
    \input streams-1.tex
  \end{center}
  \caption{Streams.\label{streamsOne}}
\end{figure}

\subsection{Format output to file\label{fstream-format} (test \#1)}

First test is a format output to a file. File stream opened for writing
and printing a set of records (interger, char and double). Each record
terminated by 0 (end-of-string).

\lstinputlisting{stream/fstream-format/str.cc}

As reference I use C program (titled as stdio at figure~\ref{streams}) that
do near the same.

\lstinputlisting{stream/fstream-format/str2.c}

\subsection{Raw output to file\label{fstream-raw} (test \#2)}

Raw write to file.

\lstinputlisting{stream/fstream-raw/str.cc}

Near the same test on pure C (stdio).

\lstinputlisting{stream/fstream-raw/str2.c}

And, as reference, unbuffered output (unistd):

\lstinputlisting{stream/fstream-raw/str3.c}

\subsection{Raw output to string\label{sstream-raw} (test \#3)}

Raw write to string stream.

\lstinputlisting{stream/sstream-raw/str.cc}

Analog on C. To be near the real life I simulate that I don't know
the maximum size of buffer.

\lstinputlisting{stream/sstream-raw/str2.c}

We see that format output to file show near the same time
for \STLport{}, \libstd{} and (even!) C \verb|fprintf|. For raw write
the \STLport's implementation is a bit better than \libstd{} (C \verb|fwrite|
show best results here, but breakaway is minimal; all has huge advantage over
unbuffered output). With raw write to string stream
(or in char buffer in case of C) the pure C variant is 3--4 times faster,
but this not what iostreams was intended for.

\section{Conclusions}

This tests show that
\begin{itemize}
  \item for processing long strings (greater than 50K) the best choice is
        ropes;
  \item if you use strings with sizes $0.5$K--$50$K then strings implementation
        from native \libstd{} is a good choice; this will be good too if you only pass
        strings as parameters, without modifictions (but this is bad programming
        technique nevertheless);
  \item if you general work is modification of strings with sizes less then $0.5$K,
        the strings from \STLport{} are for you;
  \item the time of advantage of \texttt{node\_alloc} in \STLport{} is in the past;
        progress in allocation algorithms in core system eliminate positive
        effect of \texttt{node\_alloc} in single-threaded applications and demonstrate
        significant advantage of core system allocators in multi-threaded
        applications;
  \item no valuable performance advantage of C format output over \CPP{} iosreams;
  \item unbuffered output has ugly performance (is it new fact for you?);
  \item no reasons to use \verb|node_alloc| in \STLport{}---\verb|malloc_alloc|
        show better (or significant better) results;
  \item string proxy object (aka expression template) technique is very useful.
\end{itemize}

% As about Intel$^{\mbox{\tiny\textregistered}}$ vs. AMD$^{\mbox{\tiny\textregistered}}$, no comments---see figure~\ref{intel-amd} on page~\pageref{intel-amd}.

\section{References}

\noindent
\begin{description}
  \item[STLport] \texttt{http://stlport.sourceforge.net}
  \item[GCC]     \texttt{http://gcc.gnu.org}
  \item[SGI]     \texttt{http://www.sgi.com/tech/stl/}
  \item[Complement] \texttt{http://complement.sourceforge.net}
\end{description}


\end{document}
